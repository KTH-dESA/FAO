{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\") #this is to add the avobe folder to the package directory\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nexus_tool.weap_tools as wp\n",
    "import os\n",
    "import scripts.softlinking_functions as sf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read processed schematic files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_folder = os.path.join('..', 'Jordan dashboard', 'spatial_data')\n",
    "demand = gpd.read_file(os.path.join(sp_folder, 'Demand_points.gpkg'))\n",
    "supply = gpd.read_file(os.path.join(sp_folder, 'Supply_points.gpkg'))\n",
    "pipelines = gpd.read_file(os.path.join(sp_folder, 'Pipelines.gpkg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read WEAP input data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = os.path.join('Data', 'WEAP Results', 'Dec 08', 'Reference - Climate Change.xlsm')\n",
    "data = pd.ExcelFile(data_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read required water data sheets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sheet_names = {'Supply Requirement_AG': 'Agriculture', \n",
    "               'Supply Requirement_Muni': 'Municipality', \n",
    "               'Supply Requirement_Ind': 'Industry'}\n",
    "required_demand = sf.get_demand_data(sheet_names, data, demand, 'point', '^ {}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read delivered water data sheets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sheet_names = {'Supply Delivered_AG': 'Agriculture', \n",
    "               'Supply Delivered_Muni': 'Municipality', \n",
    "               'Supply Delivered_Ind': 'Industry'}\n",
    "delivered_demand = sf.get_demand_data(sheet_names, data, demand, 'point', '^ {}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mapping elevation from spatial layer to dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "delivered_demand['elevation_m'] = delivered_demand.point.map(demand.groupby('point')['elevation_m'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read desalination plants data sheets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Red-Dead project\n",
    "sheet_names = {'RedDead': 'Desalination'}\n",
    "red_dead = sf.get_demand_data(sheet_names, data, supply, 'point', '{}')\n",
    "red_dead = red_dead.append(sf.get_demand_data(sheet_names, data, demand, 'point', '{}'))\n",
    "red_dead['point'] = 'RedDead'\n",
    "\n",
    "#Aqaba plant\n",
    "sheet_names = {'Aqaba Desal': 'Desalination'}\n",
    "aqaba_desal = sf.get_demand_data(sheet_names, data, demand, 'point', '{}')\n",
    "aqaba_desal['point'] = 'Aqaba Desal'\n",
    "\n",
    "#Merge both together\n",
    "desalination = red_dead.append(aqaba_desal)\n",
    "desalination['value'] = abs(desalination['value'])\n",
    "desalination['variable'] = desalination.variable.str.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read groundwater supply data sheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sheet_names = {'GW Pumping': 'Groundwater supply'}\n",
    "groundwater = supply.loc[supply['type']=='Groundwater supply']\n",
    "gw_supply =  sf.get_demand_data(sheet_names, data, groundwater, 'point', '^ {}')\n",
    "gw_supply['wtd_m'] = gw_supply.point.map(groundwater.groupby('point')['wtd_m'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read and process groundwater level change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sheet_names = {'Groundwater': 'Thickness'}\n",
    "gw_thickness =  sf.get_demand_data(sheet_names, data, supply, 'point', '^ {}')\n",
    "gw_thickness.rename(columns={'value': 'thickness', 'units': 'thickness_units'}, inplace=True)\n",
    "gw_thickness.drop(columns='type', inplace=True)\n",
    "\n",
    "for point in gw_thickness.point.unique():\n",
    "    _filter = (gw_thickness.point==point)\n",
    "    init_thickness = gw_thickness.loc[_filter].iloc[0].thickness\n",
    "    gw_thickness.loc[_filter, 'wtd_m'] = init_thickness - gw_thickness.loc[_filter, 'thickness'] + \\\n",
    "                                         float(groundwater.loc[groundwater.point==point,'wtd_m'].mean())\n",
    "\n",
    "#Merge datasets on groundwater supply and level change (thickness)\n",
    "gw_supply = gw_supply.merge(gw_thickness, on=['Year','Month','point','variable'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read and process pipeline supply and wastewater treatment plants data sheets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sheet_names = {'Wadis': 'River/pipeline supply'}\n",
    "surface_water =  sf.get_demand_data(sheet_names, data, supply, 'point', '{}')\n",
    "\n",
    "sheet_names = {'WWTP Inflow': 'WWTP'}\n",
    "wwtp_inflow =  sf.get_demand_data(sheet_names, data, supply, 'point', '^ {}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline flow processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl_flow = pd.DataFrame()\n",
    "\n",
    "for sheet_name in ['Pipelines', 'PumpStations']:\n",
    "    pl_data = data.parse(sheet_name, skiprows=5)\n",
    "    pl_data.rename(columns={'$Columns = Year': 'Year', ' Timestep': 'Month'}, inplace=True)\n",
    "    pl_data.columns = pl_data.columns.str.replace('\"', '')\n",
    "    pl_data.columns = pl_data.columns.str.replace('  ', ' ')\n",
    "    pl_data.columns = pl_data.columns.str.strip()\n",
    "    if sheet_name == 'Pipelines':\n",
    "        pl_data[\"PL_ZaraMain2SZ09SZ06 0 \\ Reach[Cubic Meter]\"] = pl_data[\"PL_ZaraMain2SZ09SZ06 0 \\ Headflow[Cubic Meter]\"]\n",
    "\n",
    "    pipelines_temp = sf.extract_weap(pl_data, pipelines, 'pipeline', ['Year','Month'], '^{} [0-9]')\n",
    "    pipelines_temp['units'] = [sf.extract(s, '\\[', '\\]') for s in pipelines_temp.variable]\n",
    "    pipelines_temp['variable'] = [(i).split('[')[0].strip() for i in pipelines_temp.variable]\n",
    "\n",
    "    pl_flow = pl_flow.append(pipelines_temp, sort=False)\n",
    "\n",
    "pl_flow = pl_flow.loc[~pl_flow.variable.str.contains('FR')]\n",
    "\n",
    "pl_flow['point'] = np.nan\n",
    "_vec = ~pl_flow.variable.isin(['Headflow','Reach'])\n",
    "pl_flow.loc[_vec,'point'] = pl_flow.loc[_vec,'variable']\n",
    "\n",
    "_df = pl_flow.loc[(pl_flow.Year==2020) & (pl_flow.Month==1)].groupby('pipeline').count()\n",
    "idx = _df.loc[_df.point<3].index\n",
    "_df = pl_flow.loc[pl_flow.pipeline.isin(idx)].copy()\n",
    "\n",
    "for pipeline in _df.pipeline.unique():\n",
    "    for row in pipelines.loc[pipelines.pipeline==pipeline].iterrows():\n",
    "        if (row[1].point not in _df.loc[(_df.pipeline==pipeline), 'variable'].unique()) & (row[1].type=='Diversion Outflow'):\n",
    "            pl_flow.loc[(pl_flow.pipeline==pipeline) & (pl_flow.variable=='Headflow'), 'point'] = row[1].point\n",
    "        elif (row[1].point not in _df.loc[(_df.pipeline==pipeline), 'variable'].unique()) & (row[1].type=='Tributary inflow'):\n",
    "                pl_flow.loc[(pl_flow.pipeline==pipeline) & (pl_flow.variable=='Reach'), 'point'] = row[1].point\n",
    "\n",
    "_df = pipelines[['pipeline','point','index']].groupby(['pipeline','point']).mean()\n",
    "_pl_flow = pl_flow.set_index(['pipeline','point'])\n",
    "_index = _pl_flow.index.map(_df['index'].to_dict())\n",
    "pl_flow['pl_index'] = [(i + 0.5) if i%1==0.5 else (i) for i in _index]\n",
    "pl_flow['elevation'] = pl_flow.point.map(pipelines.groupby('point')['elevation_m'].mean().to_dict())\n",
    "pl_flow['segment_length'] = pl_flow.pl_index.map(pipelines.groupby('index')['segment_length_m'].mean().to_dict())\n",
    "pl_flow['pipeline_length'] = pl_flow.pipeline.map(pipelines.groupby('pipeline')['pl_length_m'].mean().to_dict())\n",
    "\n",
    "pl_flow.dropna(subset=['point'], inplace=True)\n",
    "\n",
    "sf.enumerate_segments(pl_flow)\n",
    "sf.get_elevation_delta(pl_flow)\n",
    "\n",
    "pl_flow.loc[(pl_flow.variable=='Reach') & (pl_flow.elevation_delta!=0), 'point'] = np.nan\n",
    "pl_flow.dropna(subset=['point'], inplace=True)\n",
    "\n",
    "sf.enumerate_segments(pl_flow)\n",
    "sf.get_elevation_delta(pl_flow)\n",
    "\n",
    "_point = supply.loc[supply['type']=='River/pipeline supply'].point.unique()\n",
    "\n",
    "_pipe = pl_flow.pipeline.unique()\n",
    "pl_flow['water_use'] = 0\n",
    "for pipe in _pipe:\n",
    "    n = pl_flow.loc[(pl_flow['pipeline']==pipe)].n.unique()\n",
    "    for _n in range(1,n.max()+1):\n",
    "        value2 = np.array(pl_flow.loc[(pl_flow['pipeline']==pipe) & (pl_flow['n']==_n), 'value'])\n",
    "        value1 = np.array(pl_flow.loc[(pl_flow['pipeline']==pipe) & (pl_flow['n']==(_n-1)), 'value'])\n",
    "        pl_flow.loc[(pl_flow['pipeline']==pipe) & (pl_flow['n']==_n), 'water_use'] = abs(value1 - value2)\n",
    "\n",
    "    if pl_flow.loc[(pl_flow['pipeline']==pipe)].water_use.sum() == 0:\n",
    "        pl_flow.loc[(pl_flow['pipeline']==pipe), 'water_use'] = \\\n",
    "                                        pl_flow.loc[(pl_flow['pipeline']==pipe), 'value'].mean()/\\\n",
    "                                        pl_flow.loc[(pl_flow['pipeline']==pipe), 'value'].count()\n",
    "\n",
    "pl_flow['type'] = np.nan\n",
    "pl_flow.loc[pl_flow['point'].isin(_point), 'type'] = 'River/pipeline supply'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the processed results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_folder = os.path.join('Data', 'Processed results', 'Reference', 'Climate Change', 'level_1')\n",
    "os.makedirs(output_folder)\n",
    "\n",
    "desalination.to_csv(os.path.join(output_folder, 'desalination.csv'), index=False)\n",
    "wwtp_inflow.to_csv(os.path.join(output_folder, 'wwtp_inflow.csv'), index=False)\n",
    "surface_water.to_csv(os.path.join(output_folder, 'surface_water_supply.csv'), index=False)\n",
    "gw_supply.to_csv(os.path.join(output_folder, 'groundwater_supply.csv'), index=False)\n",
    "delivered_demand.to_csv(os.path.join(output_folder, 'delivered_demand.csv'), index=False)\n",
    "required_demand.to_csv(os.path.join(output_folder, 'required_demand.csv'), index=False)\n",
    "pl_flow.to_csv(os.path.join(output_folder, 'pipelines_flow.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_dict = {'Reference': {'Eto trend': ['DataExchange - Reference.xlsm'], \n",
    "                            'Without Eto trend': ['DataExchange - Reference - No ETo Trend.xlsm']},\n",
    "              'Improve AG eff': {'Eto trend': ['DataExchange - Improve AG eff by 10percent.xlsm',\n",
    "                                               'DataExchange - Improve AG eff by 20percent.xlsm'], \n",
    "                                 'Without Eto trend': ['DataExchange - Improve AG eff by 10percent - No ETo Trend.xlsm',\n",
    "                                                       'DataExchange - Improve AG eff by 20percent - No ETo Trend.xlsm']},\n",
    "              'New Resources': {'Eto trend': ['DataExchange - New Resources.xlsm'], \n",
    "                                'Without Eto trend': ['DataExchange - New Resources - No ETo Trend.xlsm']},\n",
    "              'Reduce NRW': {'Eto trend': ['DataExchange - Reduce NRW to 40 percent.xlsm', \n",
    "                                           'DataExchange - Reduce NRW to 20 percent.xlsm'], \n",
    "                             'Without Eto trend': ['DataExchange - Reduce NRW to 40 percent - No ETo Trend.xlsm',\n",
    "                                                   'DataExchange - Reduce NRW to 20 percent - No ETo Trend.xlsm',]}}\n",
    "\n",
    "def create_folder(path):\n",
    "    if not os.path.exists(path):\n",
    "        os.mkdir(path)\n",
    "\n",
    "raw_data = os.path.join('Data/WEAP Results', 'Dec 08')\n",
    "results_path = os.path.join('Data/WEAP Results', 'Results')\n",
    "create_folder(results_path)\n",
    "for scenario, files in files_dict.items():\n",
    "    scenario_path = os.path.join(results_path, scenario)\n",
    "    create_folder(scenario_path)\n",
    "    for folder, file in files.items():\n",
    "        folder_path = os.path.join(scenario_path, folder)\n",
    "        create_folder(folder_path)\n",
    "        for i, f in enumerate(file):\n",
    "            dst = os.path.join(folder_path, f'level_{i+1}')\n",
    "            create_folder(dst)\n",
    "            copyfile(os.path.join(raw_data, f), os.path.join(dst, 'results.xlsm'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scenarios = ['Reference', 'Improve AG eff', 'New Resources', 'Reduce NRW']\n",
    "scenarios = ['Reference']\n",
    "processed_data = os.path.join('Data/WEAP Results', 'Processed data')\n",
    "for scenario in scenarios:\n",
    "    scenario_folder = os.path.join(results_path, scenario)\n",
    "    create_folder(os.path.join(processed_data, scenario))\n",
    "    for sub_scenario in os.listdir(scenario_folder):\n",
    "        sub_scenario_folder = os.path.join(scenario_folder, sub_scenario)\n",
    "        create_folder(os.path.join(processed_data, scenario, sub_scenario))\n",
    "        for level in os.listdir(sub_scenario_folder):\n",
    "            output_folder = os.path.join(processed_data, scenario, sub_scenario, level)\n",
    "            create_folder(output_folder)\n",
    "            file = os.path.join(sub_scenario_folder, level, 'results.xlsm')\n",
    "            data = pd.ExcelFile(file)\n",
    "\n",
    "            sheet_names = {'Supply Requirement_AG': 'Agriculture', \n",
    "                           'Supply Requirement_Muni': 'Municipality', \n",
    "                           'Supply Requirement_Ind': 'Industry'}\n",
    "            required_demand = sf.get_demand_data(sheet_names, data, demand, 'point', '^ {}')\n",
    "            required_demand['elevation_m'] = required_demand.point.map(demand.groupby('point')['elevation_m'].mean())\n",
    "\n",
    "            demand['type'] = demand.point.map(required_demand.groupby('point').agg({'type': 'first'}).type)\n",
    "\n",
    "            sheet_names = {'Supply Delivered_AG': 'Agriculture', \n",
    "                           'Supply Delivered_Muni': 'Municipality', \n",
    "                           'Supply Delivered_Ind': 'Industry'}\n",
    "            delivered_demand = sf.get_demand_data(sheet_names, data, demand, 'point', '^ {}')\n",
    "            delivered_demand['elevation_m'] = delivered_demand.point.map(demand.groupby('point')['elevation_m'].mean())\n",
    "\n",
    "            demand.loc[demand['type'].isna(), 'type'] = demand.loc[demand['type'].isna(), 'point'].map(delivered_demand.groupby('point').agg({'type': 'first'}).type)\n",
    "\n",
    "            sheet_names = {'RedDead': 'Desalination'}\n",
    "            red_dead = sf.get_demand_data(sheet_names, data, supply, 'point', '{}')\n",
    "            red_dead = red_dead.append(sf.get_demand_data(sheet_names, data, demand, 'point', '{}'))\n",
    "            red_dead['point'] = 'RedDead'\n",
    "            sheet_names = {'Aqaba Desal': 'Desalination'}\n",
    "            aqaba_desal = sf.get_demand_data(sheet_names, data, demand, 'point', '{}')\n",
    "            aqaba_desal['point'] = 'Aqaba Desal'\n",
    "            desalination = red_dead.append(aqaba_desal)\n",
    "            desalination['value'] = abs(desalination['value'])\n",
    "            desalination['variable'] = desalination.variable.str.strip()\n",
    "            \n",
    "            sheet_names = {'GW Pumping': 'Groundwater supply'}\n",
    "            gw_demand =  sf.get_demand_data(sheet_names, data, groundwater, 'point', '^ {}')\n",
    "            gw_demand['wtd_m'] = gw_demand.point.map(groundwater.groupby('point')['wtd_m'].mean())\n",
    "\n",
    "            sheet_names = {'Groundwater': 'Thickness'}\n",
    "            gw_thickness =  sf.get_demand_data(sheet_names, data, supply, 'point', '^ {}')\n",
    "            gw_thickness.rename(columns={'value': 'thickness', 'units': 'thickness_units'}, inplace=True)\n",
    "            gw_thickness.drop(columns='type', inplace=True)\n",
    "\n",
    "            for point in gw_thickness.point.unique():\n",
    "                _filter = (gw_thickness.point==point)\n",
    "                init_thickness = gw_thickness.loc[_filter].iloc[0].thickness\n",
    "                gw_thickness.loc[_filter, 'wtd'] = init_thickness - gw_thickness.loc[_filter, 'thickness'] + \\\n",
    "                                                   float(groundwater.loc[groundwater.point==point,'wtd_m'])\n",
    "            gw_demand = gw_demand.merge(gw_thickness, on=['Year','Month','point','variable'])\n",
    "\n",
    "            sheet_names = {'Wadis': 'River/pipeline supply'}\n",
    "            surface_water =  sf.get_demand_data(sheet_names, data, supply, 'point', '{}')\n",
    "\n",
    "            sheet_names = {'WWTP Inflow': 'WWTP'}\n",
    "            wwtp_inflow =  sf.get_demand_data(sheet_names, data, supply, 'point', '^ {}')\n",
    "\n",
    "            pl_flow = pd.DataFrame()\n",
    "\n",
    "            for sheet_name in ['Pipelines', 'PumpStations']:\n",
    "                pl_data = data.parse(sheet_name, skiprows=5)\n",
    "                pl_data.rename(columns={'$Columns = Year': 'Year', ' Timestep': 'Month'}, inplace=True)\n",
    "                pl_data.columns = pl_data.columns.str.replace('\"', '')\n",
    "                pl_data.columns = pl_data.columns.str.replace('  ', ' ')\n",
    "                pl_data.columns = pl_data.columns.str.strip()\n",
    "                if sheet_name == 'Pipelines':\n",
    "                    pl_data[\"PL_ZaraMain2SZ09SZ06 0 \\ Reach[Cubic Meter]\"] = pl_data[\"PL_ZaraMain2SZ09SZ06 0 \\ Headflow[Cubic Meter]\"]\n",
    "\n",
    "                df_pl_temp = extract_weap(pl_data, df_pl, 'pipeline', ['Year','Month'], '^{} [0-9]')\n",
    "                df_pl_temp['units'] = [extract(s, '\\[', '\\]') for s in df_pl_temp.variable]\n",
    "                df_pl_temp['variable'] = [(i).split('[')[0].strip() for i in df_pl_temp.variable]\n",
    "\n",
    "                pl_flow = pl_flow.append(df_pl_temp, sort=False)\n",
    "\n",
    "            \n",
    "            pl_flow = pl_flow.loc[~pl_flow.variable.str.contains('FR')]\n",
    "\n",
    "            pl_flow['point'] = np.nan\n",
    "            _vec = ~pl_flow.variable.isin(['Headflow','Reach'])\n",
    "            pl_flow.loc[_vec,'point'] = pl_flow.loc[_vec,'variable']\n",
    "\n",
    "            _df = pl_flow.loc[(pl_flow.Year==2020) & (pl_flow.Month==1)].groupby('pipeline').count()\n",
    "            idx = _df.loc[_df.point<3].index\n",
    "            _df = pl_flow.loc[pl_flow.pipeline.isin(idx)].copy()\n",
    "               \n",
    "\n",
    "            for pipeline in _df.pipeline.unique():\n",
    "                for row in df_pl.loc[df_pl.pipeline==pipeline].iterrows():\n",
    "                    if (row[1].point not in _df.loc[(_df.pipeline==pipeline), 'variable'].unique()) & (row[1].type=='Diversion Outflow'):\n",
    "                        pl_flow.loc[(pl_flow.pipeline==pipeline) & (pl_flow.variable=='Headflow'), 'point'] = row[1].point\n",
    "                    elif (row[1].point not in _df.loc[(_df.pipeline==pipeline), 'variable'].unique()) & (row[1].type=='Tributary inflow'):\n",
    "                            pl_flow.loc[(pl_flow.pipeline==pipeline) & (pl_flow.variable=='Reach'), 'point'] = row[1].point\n",
    "\n",
    "            \n",
    "            _df = df_pl[['pipeline','point','index']].groupby(['pipeline','point']).mean()\n",
    "            # _df = _df.reset_index().set_index('point')\n",
    "            _pl_flow = pl_flow.set_index(['pipeline','point'])\n",
    "            _index = _pl_flow.index.map(_df['index'].to_dict())\n",
    "            pl_flow['pl_index'] = [(i + 0.5) if i%1==0.5 else (i) for i in _index]\n",
    "            pl_flow['elevation'] = pl_flow.point.map(df_pl.groupby('point')['elevation_m'].mean().to_dict())\n",
    "            pl_flow['segment_length'] = pl_flow.pl_index.map(df_pl.groupby('index')['segment_length_m'].mean().to_dict())\n",
    "            pl_flow['pipeline_length'] = pl_flow.pipeline.map(df_pl.groupby('pipeline')['pl_length_m'].mean().to_dict())\n",
    "\n",
    "            pl_flow.dropna(subset=['point'], inplace=True)\n",
    "\n",
    "            enumerate_segments(pl_flow)\n",
    "            get_elevation_delta(pl_flow)\n",
    "\n",
    "            pl_flow.loc[(pl_flow.variable=='Reach') & (pl_flow.elevation_delta!=0), 'point'] = np.nan\n",
    "            pl_flow.dropna(subset=['point'], inplace=True)\n",
    "\n",
    "            enumerate_segments(pl_flow)\n",
    "            get_elevation_delta(pl_flow)\n",
    "\n",
    "            _point = supply.loc[supply['type']=='River/pipeline supply'].point.unique()\n",
    "#             _pipe = pl_flow.loc[pl_flow['point'].isin(_point)].pipeline.unique()\n",
    "            _pipe = pl_flow.pipeline.unique()\n",
    "            pl_flow['water_use'] = 0\n",
    "            for pipe in _pipe:\n",
    "#                 n = pl_flow.loc[(pl_flow['pipeline']==pipe) & pl_flow['point'].isin(_point)].n.unique()\n",
    "                n = pl_flow.loc[(pl_flow['pipeline']==pipe)].n.unique()\n",
    "                for _n in range(1,n.max()+1):\n",
    "                    \n",
    "                    value2 = np.array(pl_flow.loc[(pl_flow['pipeline']==pipe) & (pl_flow['n']==_n), 'value'])\n",
    "                    value1 = np.array(pl_flow.loc[(pl_flow['pipeline']==pipe) & (pl_flow['n']==(_n-1)), 'value'])\n",
    "                    pl_flow.loc[(pl_flow['pipeline']==pipe) & (pl_flow['n']==_n), 'water_use'] = abs(value1 - value2)\n",
    "                \n",
    "                if pl_flow.loc[(pl_flow['pipeline']==pipe)].water_use.sum() == 0:\n",
    "                    pl_flow.loc[(pl_flow['pipeline']==pipe), 'water_use'] = \\\n",
    "                                                    pl_flow.loc[(pl_flow['pipeline']==pipe), 'value'].mean()/\\\n",
    "                                                    pl_flow.loc[(pl_flow['pipeline']==pipe), 'value'].count()\n",
    "            \n",
    "            \n",
    "            pl_flow['type'] = np.nan\n",
    "            pl_flow.loc[pl_flow['point'].isin(_point), 'type'] = 'River/pipeline supply'\n",
    "            \n",
    "            desalination.to_csv(os.path.join(output_folder, 'Desalination.csv'), index=False)\n",
    "            wwtp_inflow.to_csv(os.path.join(output_folder, 'WWTP_inflow.csv'), index=False)\n",
    "            surface_water.to_csv(os.path.join(output_folder, 'Surface_water_supply.csv'), index=False)\n",
    "            gw_demand.to_csv(os.path.join(output_folder, 'Groundwater_supply.csv'), index=False)\n",
    "            delivered_demand.to_csv(os.path.join(output_folder, 'Delivered_demand.csv'), index=False)\n",
    "            required_demand.to_csv(os.path.join(output_folder, 'Required_demand.csv'), index=False)\n",
    "            pl_flow.to_csv(os.path.join(output_folder, 'Pipelines_flow.csv'), index=False)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_demand_data(sheet_names, data, spatial_data, variable, regex):\n",
    "    merged_data = pd.DataFrame()\n",
    "    for sheet_name, value in sheet_names.items():\n",
    "        data_temp = data.parse(sheet_name, skiprows=5)\n",
    "        data_temp.columns = data_temp.columns.str.replace('\"', '')\n",
    "        merged_data_temp = extract_weap(data_temp, spatial_data, variable, ['Year'], regex)\n",
    "        merged_data_temp['units'] = 'kg'\n",
    "        merged_data_temp['variable'] = [i.split('[')[0] for i in merged_data_temp.variable]\n",
    "        merged_data_temp['type'] = value\n",
    "        merged_data_temp.loc[merged_data_temp['type']=='variable', 'type'] = merged_data_temp.loc[merged_data_temp['type']=='variable', 'variable']\n",
    "\n",
    "        merged_data = merged_data.append(merged_data_temp, sort=False)\n",
    "        \n",
    "    return merged_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_weap(results, spacial_data, points, melt_on, regex):\n",
    "    df = pd.DataFrame()\n",
    "    for key, row in spacial_data.groupby(points):\n",
    "        df_temp = results.filter(regex=regex.format(key.replace('Agriculture', 'Agri').replace(' ',''))).copy()\n",
    "        df_temp.columns = [i.split('\\\\')[-1] for i in df_temp.columns]\n",
    "        df_temp[points] = key\n",
    "        for i in melt_on:\n",
    "            df_temp[i] = results[i]\n",
    "        melt_vars = melt_on.copy()\n",
    "        melt_vars.append(points)\n",
    "        df = df.append(df_temp.melt(id_vars=melt_vars), sort=False)\n",
    "    return df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = r\"Data\\WEAP Results\\Crop Production.xlsx\"\n",
    "data = pd.ExcelFile(file)\n",
    "sheet_names = {'WEAP Export': 'Crop production'}\n",
    "crop_production =  get_demand_data(sheet_names, data, demand, 'point', '^{}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = crop_production.loc[crop_production.point.str.contains('AQ')]\n",
    "df = df.groupby(['Year', 'variable']).sum().reset_index()\n",
    "fig = px.bar(df, x='Year', y='value', color='variable')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demand.point.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = r'../Jordan dashboard/spatial_data/'\n",
    "demand.to_file(f'{folder}Demand_points.gpkg', driver='GPKG')\n",
    "supply.to_file(f'{folder}Supply_points.gpkg', driver='GPKG')\n",
    "df_pl.to_file(f'{folder}Pipelines.gpkg', driver='GPKG')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
