{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\") #this is to add the avobe folder to the package directory\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nexus_tool.weap_tools as wp\n",
    "import os\n",
    "import scripts.softlinking_functions as sf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read processed schematic files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "gis_folder = os.path.join('data', 'GIS', 'Processed layers')\n",
    "demand = gpd.read_file(os.path.join(gis_folder, 'Demand_points.gpkg'))\n",
    "supply = gpd.read_file(os.path.join(gis_folder, 'Supply_points.gpkg'))\n",
    "pipelines = gpd.read_file(os.path.join(gis_folder, 'Pipelines.gpkg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read WEAP input data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = os.path.join('Data', 'WEAP Results', 'March 2021', 'WEAPResults - Reference.xlsx')\n",
    "data = pd.ExcelFile(data_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read required water data sheets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sheet_names = {'Supply Requirement_AG': 'Agriculture', \n",
    "               'Supply Requirement_Muni': 'Municipality', \n",
    "               'Supply Requirement_Ind': 'Industry'}\n",
    "required_demand = sf.get_data(sheet_names, data, demand, 'point', '{}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read delivered water data sheets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sheet_names = {'Supply Delivered_AG': 'Agriculture', \n",
    "               'Supply Delivered_Muni': 'Municipality', \n",
    "               'Supply Delivered_Ind': 'Industry'}\n",
    "delivered_demand = sf.get_data(sheet_names, data, demand, 'point', '{}')\n",
    "\n",
    "delivered_demand['point'] = delivered_demand['point'].str.replace('BALQA', 'BQ')\n",
    "delivered_demand['point'] = delivered_demand['point'].str.replace('MADABA', 'MD')\n",
    "delivered_demand['point'] = delivered_demand['point'].str.replace('JERASH', 'JA')\n",
    "\n",
    "governorates = {'AJ': 'Ajlun', 'AM': 'Amman', 'AQ': 'Aqaba', 'BQ': 'Balqa', \n",
    "                'IR': 'Irbid', 'JA': 'Jarash', 'KA': 'Karak', 'MA': 'Ma`an', \n",
    "                'MD': 'Madaba', 'MF': 'Mafraq', 'TA': 'Tafilah', 'ZA': 'Zarqa'}\n",
    "delivered_demand['Governorate'] = [governorates[i[0:2]] for i in delivered_demand['point']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mapping elevation from spatial layer to dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "delivered_demand['elevation_m'] = delivered_demand.point.map(demand.groupby('point')['elevation_m'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read desalination plants data sheets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Red-Dead project\n",
    "sheet_names = {'RedDead': 'Desalination'}\n",
    "red_dead = sf.get_data(sheet_names, data, supply, 'point', '{}')\n",
    "red_dead = red_dead.append(sf.get_data(sheet_names, data, demand, 'point', '{}'))\n",
    "red_dead['point'] = 'RedDead'\n",
    "\n",
    "#Aqaba plant\n",
    "sheet_names = {'Aqaba Desal': 'Desalination'}\n",
    "aqaba_desal = sf.get_data(sheet_names, data, demand, 'point', '{}')\n",
    "aqaba_desal['point'] = 'Aqaba Desal'\n",
    "\n",
    "#Merge both together\n",
    "desalination = red_dead.append(aqaba_desal)\n",
    "desalination['value'] = abs(desalination['value'])\n",
    "desalination['variable'] = desalination.variable.str.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read groundwater supply data sheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sheet_names = {'GW Pumping': 'Groundwater supply'}\n",
    "groundwater = supply.loc[supply['type']=='Groundwater supply']\n",
    "gw_supply =  sf.get_data(sheet_names, data, groundwater, 'point', '{}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read and process groundwater level change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sheet_names = {'Groundwater': 'Thickness'}\n",
    "gw_thickness =  sf.get_data(sheet_names, data, supply, 'point', '{}')\n",
    "gw_thickness.rename(columns={'value': 'thickness', 'units': 'thickness_units'}, inplace=True)\n",
    "gw_thickness.drop(columns='type', inplace=True)\n",
    "\n",
    "for point in gw_thickness.point.unique():\n",
    "    _filter = (gw_thickness.point==point)\n",
    "    init_thickness = gw_thickness.loc[_filter].iloc[0].thickness\n",
    "    gw_thickness.loc[_filter, 'wtd_m'] = init_thickness - gw_thickness.loc[_filter, 'thickness'] + \\\n",
    "                                         float(groundwater.loc[groundwater.point==point,'wtd_m'].mean())\n",
    "\n",
    "#Merge datasets on groundwater supply and level change (thickness)\n",
    "gw_supply = gw_supply.merge(gw_thickness, on=['Year','Month','point','variable'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read and process pipeline supply and wastewater treatment plants data sheets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sheet_names = {'Wadis': 'River/pipeline supply'}\n",
    "surface_water =  sf.get_data(sheet_names, data, supply, 'point', '{}')\n",
    "\n",
    "sheet_names = {'WWTP Inflow': 'WWTP'}\n",
    "wwtp_inflow =  sf.get_data(sheet_names, data, supply, 'point', '{}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline flow processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sheet_names = {'Pipelines': 'conveyance', \n",
    "               'PumpStations': 'conveyance'}\n",
    "pl_flow = sf.get_data(sheet_names, data, pipelines, 'pipeline', '^{} [0-9]')\n",
    "\n",
    "pl_flow = pl_flow.loc[~pl_flow.variable.str.contains('FR')]\n",
    "\n",
    "pl_flow['point'] = np.nan\n",
    "_vec = ~pl_flow.variable.isin(['Headflow','Reach'])\n",
    "pl_flow.loc[_vec,'point'] = pl_flow.loc[_vec,'variable']\n",
    "\n",
    "_df = pl_flow.loc[(pl_flow.Year==2020) & (pl_flow.Month==1)].groupby('pipeline').count()\n",
    "idx = _df.loc[_df.point<3].index\n",
    "_df = pl_flow.loc[pl_flow.pipeline.isin(idx)].copy()\n",
    "\n",
    "for pipeline in _df.pipeline.unique():\n",
    "    for row in pipelines.loc[pipelines.pipeline==pipeline].iterrows():\n",
    "        if (row[1].point not in _df.loc[(_df.pipeline==pipeline), 'variable'].unique()) & (row[1].type=='Diversion Outflow'):\n",
    "            pl_flow.loc[(pl_flow.pipeline==pipeline) & (pl_flow.variable=='Headflow'), 'point'] = row[1].point\n",
    "        elif (row[1].point not in _df.loc[(_df.pipeline==pipeline), 'variable'].unique()) & (row[1].type=='Tributary inflow'):\n",
    "                pl_flow.loc[(pl_flow.pipeline==pipeline) & (pl_flow.variable=='Reach'), 'point'] = row[1].point\n",
    "\n",
    "_df = pipelines[['pipeline','point','index']].groupby(['pipeline','point']).mean()\n",
    "_pl_flow = pl_flow.set_index(['pipeline','point'])\n",
    "_index = _pl_flow.index.map(_df['index'].to_dict())\n",
    "pl_flow['pl_index'] = [(i + 0.5) if i%1==0.5 else (i) for i in _index]\n",
    "pl_flow['elevation'] = pl_flow.point.map(pipelines.groupby('point')['elevation_m'].mean().to_dict())\n",
    "pl_flow['segment_length'] = pl_flow.pl_index.map(pipelines.groupby('index')['segment_length_m'].mean().to_dict())\n",
    "pl_flow['pipeline_length'] = pl_flow.pipeline.map(pipelines.groupby('pipeline')['pl_length_m'].mean().to_dict())\n",
    "\n",
    "pl_flow.dropna(subset=['point'], inplace=True)\n",
    "\n",
    "sf.enumerate_segments(pl_flow)\n",
    "sf.get_elevation_delta(pl_flow)\n",
    "\n",
    "pl_flow.loc[(pl_flow.variable=='Reach') & (pl_flow.elevation_delta!=0), 'point'] = np.nan\n",
    "pl_flow.dropna(subset=['point'], inplace=True)\n",
    "\n",
    "sf.enumerate_segments(pl_flow)\n",
    "sf.get_elevation_delta(pl_flow)\n",
    "\n",
    "_point = supply.loc[supply['type']=='River/pipeline supply'].point.unique()\n",
    "\n",
    "_pipe = pl_flow.pipeline.unique()\n",
    "pl_flow['water_use'] = 0\n",
    "for pipe in _pipe:\n",
    "    n = pl_flow.loc[(pl_flow['pipeline']==pipe)].n.unique()\n",
    "    for _n in range(1,n.max()+1):\n",
    "        value2 = np.array(pl_flow.loc[(pl_flow['pipeline']==pipe) & (pl_flow['n']==_n), 'value'])\n",
    "        value1 = np.array(pl_flow.loc[(pl_flow['pipeline']==pipe) & (pl_flow['n']==(_n-1)), 'value'])\n",
    "        pl_flow.loc[(pl_flow['pipeline']==pipe) & (pl_flow['n']==_n), 'water_use'] = abs(value1 - value2)\n",
    "\n",
    "    if pl_flow.loc[(pl_flow['pipeline']==pipe)].water_use.sum() == 0:\n",
    "        pl_flow.loc[(pl_flow['pipeline']==pipe), 'water_use'] = \\\n",
    "                                        pl_flow.loc[(pl_flow['pipeline']==pipe), 'value'].mean()/\\\n",
    "                                        pl_flow.loc[(pl_flow['pipeline']==pipe), 'value'].count()\n",
    "\n",
    "pl_flow['type'] = np.nan\n",
    "pl_flow.loc[pl_flow['point'].isin(_point), 'type'] = 'River/pipeline supply'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read crop production data sheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sheet_names = {'Production': 'Crop production'}\n",
    "crop_production =  sf.get_data(sheet_names, data, demand, 'point', '^{}', {'Unnamed: 0': 'Year'}, ['Year'])\n",
    "\n",
    "crop_production['variable'] = crop_production['variable'].str.replace('Summer ', '')\n",
    "crop_production['variable'] = crop_production['variable'].str.replace('Winter ', '')\n",
    "crop_production['Governorate'] = [governorates[i[0:2]] for i in crop_production['point']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the processed results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_folder = os.path.join('Data', 'Processed results', 'Reference', 'Climate Change', 'level_1')\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "sf.save_dataframe(desalination, 2020, 2050, output_folder, 'desalination.csv')\n",
    "sf.save_dataframe(wwtp_inflow, 2020, 2050, output_folder, 'wwtp_inflow.csv')\n",
    "# sf.save_dataframe(surface_water, 2020, 2050, output_folder, 'surface_water_supply.csv')\n",
    "sf.save_dataframe(gw_supply, 2020, 2050, output_folder, 'groundwater_supply.csv')\n",
    "sf.save_dataframe(pl_flow, 2020, 2050, output_folder, 'pipelines_flow.csv')\n",
    "\n",
    "output_folder = os.path.join('..', 'Jordan dashboard', 'data_test', 'Reference', 'Climate Change', 'level_1')\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "sf.save_dataframe(crop_production, 2020, 2050, output_folder, 'crop_production.csv')\n",
    "sf.save_dataframe(delivered_demand, 2020, 2050, output_folder, 'water_delivered.csv')\n",
    "sf.save_dataframe(required_demand, 2020, 2050, output_folder, 'water_requirements.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
